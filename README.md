# survey-deep-gradient-compression


## Gradient Compression (Sparsification)
* [ ] [Chen, Chia-Yu, et al. "Scalecom: Scalable sparsified gradient compression for communication-efficient distributed training." arXiv preprint arXiv:2104.11125 (2021)](https://arxiv.org/pdf/2104.11125.pdf)
* [ ] [Shi, Shaohuai, et al. "Communication-efficient distributed deep learning with merged gradient sparsification on GPUs." IEEE INFOCOM 2020-IEEE Conference on Computer Communications. IEEE, 2020.](https://www.comp.hkbu.edu.hk/~chxw/papers/infocom_2020_MGS.pdf)
* [ ] [Mishchenko, Konstantin, Filip Hanzely, and Peter Richt√°rik. "99% of worker-master communication in distributed optimization is not needed." Conference on Uncertainty in Artificial Intelligence. PMLR, 2020.](http://proceedings.mlr.press/v124/mishchenko20a/mishchenko20a.pdf)
* [ ] [Shi, Shaohuai, et al. "A distributed synchronous SGD algorithm with global Top-k sparsification for low bandwidth networks." 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS). IEEE, 2019.](https://arxiv.org/pdf/1901.04359.pdf)
* [ ] [Shi, Shaohuai, et al. "A Convergence Analysis of Distributed SGD with Communication-Efficient Gradient Sparsification." IJCAI. 2019.](https://www.ijcai.org/Proceedings/2019/0473.pdf)
* [ ] [Alistarh, Dan, et al. "The convergence of sparsified gradient methods." arXiv preprint arXiv:1809.10505 (2018).](https://arxiv.org/pdf/1809.10505.pdf)
